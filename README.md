# ğŸ’¬ LLM Miniâ€‘Project â€” YouTube Comment Generator with QLoRA Fineâ€‘Tuning

[![Python](https://img.shields.io/badge/Python-3.10+-blue.svg)]()
[![Framework](https://img.shields.io/badge/Framework-Transformers%20%7C%20PEFT-orange)]()
[![Model](https://img.shields.io/badge/Model-Mistral--7B--Instruct-red.svg)]()
[![License: MIT](https://img.shields.io/badge/License-MIT-green.svg)]()

---

## ğŸ“Œ Overview
This project implements a **YouTube reply generator** powered by the **Mistralâ€‘7Bâ€‘Instruct** large language model, fineâ€‘tuned using **QLoRA** for efficient adaptation on consumer hardware.

The model generates **contextually relevant, humanâ€‘like replies** to YouTube comments, trained on a curated dataset of real commentâ€“reply pairs.  
For an **enhanced RAGâ€‘based version**, see the [improved repository](https://github.com/MAvRK7/Mini-project-YouTube-comment-generator-improved-with-RAG).

---

## ğŸ§  Key Features
- **Base Model**: [Mistralâ€‘7Bâ€‘Instruct](https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.1) â€” instructionâ€‘tuned LLM for highâ€‘quality text generation.
- **Fineâ€‘Tuning Method**: [QLoRA](https://arxiv.org/abs/2305.14314) â€” memoryâ€‘efficient lowâ€‘rank adaptation for large models.
- **Dataset**: [`shawhin/shawgpt-youtube-comments`](https://huggingface.co/datasets/shawhin/shawgpt-youtube-comments) from Hugging Face.
- **Libraries**: `transformers`, `peft`, `datasets`, `bitsandbytes` for 4â€‘bit quantization.
- **Open Source**: Fully reproducible pipeline with publicly available tools and data.

---

## ğŸ›° Dataset
- **Source**: Hugging Face dataset `shawhin/shawgpt-youtube-comments`
- **Content**: YouTube comments paired with humanâ€‘written replies
- **Preprocessing**:
  - Text cleaning (removing emojis, HTML tags, excessive whitespace)
  - Tokenization with modelâ€‘specific tokenizer
  - Train/validation split for fineâ€‘tuning

---

## ğŸ§ª Methodology

### **1. Data Preparation**
- Load dataset from Hugging Face
- Apply preprocessing and tokenization
- Format into instructionâ€“response pairs for supervised fineâ€‘tuning

### **2. Model Setup**
- Load **Mistralâ€‘7Bâ€‘Instruct** in 4â€‘bit precision using `bitsandbytes`
- Apply **QLoRA adapters** via `peft` for parameterâ€‘efficient fineâ€‘tuning

### **3. Training**
- **Loss Function**: Crossâ€‘entropy over token predictions
- **Optimizer**: AdamW with weight decay
- **Learning Rate Schedule**: Cosine decay with warmup
- **Batch Size**: Optimized for GPU memory via gradient accumulation
- **Epochs**: Tuned for convergence without overfitting

### **4. Inference**
- Input: Raw YouTube comment text
- Output: Contextually relevant, stylistically appropriate reply
- Decoding: Beam search or nucleus sampling for diversity control

---

## ğŸ“Š Example Output

**Input Comment**:  
> "This tutorial was super helpful, thanks!"

**Generated Reply**:  
> "Glad you found it useful! Let me know if you have any questions or need more examples."

---

## ğŸš€ Key Takeaways
- **QLoRA** enables fineâ€‘tuning of large models like Mistralâ€‘7B on a single GPU without sacrificing quality.
- Domainâ€‘specific fineâ€‘tuning yields **more relevant and engaging replies** than generic LLMs.
- The pipeline is **fully reproducible** and adaptable to other conversational domains.

---

## ğŸ“‚ Repository Structure
- `train.py` â€” Fineâ€‘tuning script with QLoRA integration
- `inference.py` â€” Script for generating replies from trained model
- `requirements.txt` â€” Python dependencies
- `notebooks/` â€” Jupyter notebooks for experimentation

---

## ğŸ’¡ Skills Demonstrated
- Large Language Model fineâ€‘tuning (QLoRA, PEFT)
- Efficient training with 4â€‘bit quantization
- Dataset preprocessing for conversational AI
- Hugging Face `transformers` & `datasets` integration
- Deploymentâ€‘ready inference pipeline

---

## ğŸ›  Installation

**Prerequisites**:
- Python 3.10+
- CUDAâ€‘enabled GPU (for training)

---

## Workflow Diagram

<img width="210" height="630" alt="image" src="https://github.com/user-attachments/assets/a9d83be3-cad2-4d79-b690-4edc2b82940c" />

---

## ğŸ¤ Contributing
Contributions are welcome! Please open an issue or submit a pull request.

---

## ğŸ™ Acknowledgments
- Mistral AI for the base model

- Hugging Face for datasets and libraries

- shawhin/shawgpt-youtube-comments dataset



